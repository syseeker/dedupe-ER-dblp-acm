{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  This notebook performs the following tasks based on [DBLP_ACM](http://dbs.uni-leipzig.de/de/research/projects/object_matching/fever/benchmark_datasets_for_entity_resolution) dataset:\n",
    "\n",
    "#### 1. Loads and (optionally) converts data to UTF-8 format.\n",
    "#### 2. Provides an overview of the two datasets.\n",
    "#### 3. Cleans and prepares data to dedupe compliance format.\n",
    "#### 4. Performs record linkage.\n",
    "#### 5. Evaluates record linkage results with ground truth.\n",
    "#### 6. Plots statistical results from the record linkage.\n",
    "#### 7. Selects the best result and generates network graph.\n",
    "#### 8. Provides an overview of graph analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "TODO:\n",
    "    \n",
    "    - [] comment code\n",
    "    - [] check data conversion accuracy\n",
    "    - [] to leverage previously trained data from active learning, write fout_training to the same file in every run.\n",
    "    - [] use different color palette to plot_mean_bar()\n",
    "    - [] draw interactive graph with bokeh\n",
    "    - [] create library package for sna_\n",
    "    \n",
    "    \n",
    "    \n",
    "---------\n",
    "KNOWN ISSUES:\n",
    "\n",
    "- If dedupe does not response after matching record, click Notebook -> Restart Kernel and Notebook -> Run All Cells, the matching output will be ready at output folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---------------------------\n",
    "### Configure the following parameters for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure number of run and initial run index\n",
    "NUM_RUN                      = 3\n",
    "UNIQUE_RUN_ID                = 10001\n",
    "\n",
    "# Configure data conversion\n",
    "CONVERT_DATA                 = False               # Check data encoding format and convert to utf-8 if True.\n",
    "IS_DATASET1_CONVERTED        = False               # Dedupe will read from utf-8 input file instead of original folder if True\n",
    "IS_DATASET2_CONVERTED        = False               # Dedupe will read from utf-8 input file instead of original folder if True\n",
    "READ_CONVERTED_DATASET1      = True                # Dedupe will read from utf-8 input file instead of original folder if True\n",
    "READ_CONVERTED_DATASET2      = True                # Dedupe will read from utf-8 input file instead of original folder if True\n",
    "\n",
    "# Configure LSH params for name matching\n",
    "SIM_THRES                    = 0.1    # 0.5\n",
    "NUM_PERM                     = 128    # 128\n",
    "\n",
    "# Configure GEPHI graph\n",
    "GEN_GRAPH                    = False                # Generate GML if True, else read existing author_paper_gml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import chardet\n",
    "\n",
    "import re\n",
    "import unicodecsv as csv\n",
    "from unidecode import unidecode\n",
    "\n",
    "import os\n",
    "import dedupe\n",
    "\n",
    "import codecs\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "import networkx as nx\n",
    "from sna_graph import SNA_Graph\n",
    "from sna_connected_component import SNA_ConnectedComponent\n",
    "from sna_analytics import SNA_Analytics\n",
    "\n",
    "import holoviews as hv\n",
    "import bokeh as bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input/output directory and files\n",
    "acm_csv          = \"ACM.csv\"\n",
    "dblp_csv         = \"DBLP2.csv\"\n",
    "fin_dir          = \"data/DBLP_ACM/\"\n",
    "fout_dir         = \"output/DBLP_ACM/\"\n",
    "fin_set1         = fin_dir + acm_csv\n",
    "fin_set2         = fin_dir + dblp_csv\n",
    "futf_set1        = fout_dir + 'utf8_' + acm_csv\n",
    "futf_set2        = fout_dir + 'utf8_' + dblp_csv\n",
    "\n",
    "# Dedupe default file name\n",
    "fout_res         = fout_dir + 'data_matching_output.csv'\n",
    "fout_training    = fout_dir + 'data_matching_training.json'\n",
    "fout_settings    = fout_dir + 'data_matching_learned_settings'\n",
    "\n",
    "# DBLP_ACM ground truth & matching results\n",
    "fground          = fin_dir + 'DBLP-ACM_perfectMapping.csv'\n",
    "fstats           = fout_dir + 'matching_stats.csv'\n",
    "\n",
    "# Gephi network graph\n",
    "fres_dedupe             = fout_dir + 'data_matching_output.csv'   # default best results\n",
    "author_paper_gml        = fout_dir + 'author_paper.gml'\n",
    "connected_component_gml = fout_dir + 'connected_component'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDedupeOutputFilename(run_id):\n",
    "    fout_res         = fout_dir + 'data_matching_output_' + str(run_id) + '.csv'\n",
    "    fout_training    = fout_dir + 'data_matching_training_' + str(run_id) + '.json'\n",
    "    fout_settings    = fout_dir + 'data_matching_learned_settings_' + str(run_id)\n",
    "    return [fout_res, fout_training, fout_settings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section loads and converts data to UTF-8 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadRawData(infile):\n",
    "    rawdata = io.open(infile, 'rb').read() \n",
    "    return rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkIsUtf8(data):\n",
    "    result = chardet.detect(data) \n",
    "    charenc = result['encoding']\n",
    "    if charenc != 'utf8' and charenc != 'utf-8':\n",
    "        return [False, charenc]\n",
    "    else:\n",
    "        return [True, charenc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToUtf8(data, charenc):\n",
    "    print (\"Converting \" + charenc + \" to UTF-8\")\n",
    "    data = data.decode(charenc).encode(\"utf-8\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(infile):\n",
    "    data = loadRawData(infile)\n",
    "    [is_utf8, charenc] = checkIsUtf8(data)\n",
    "    if not is_utf8:\n",
    "        data = convertToUtf8(data, charenc)\n",
    "    return data\n",
    "\n",
    "# data_1 = loadData(fin_set1)\n",
    "# data_2 = loadData(fin_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertNWriteToUtf8(data, charenc, outfile):\n",
    "    data = data.decode(charenc).encode('utf-8')\n",
    "    io.open(outfile, 'wb').write(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToUtf8(infile, outfile):\n",
    "    data = loadRawData(infile)\n",
    "    [is_utf8, charenc] = checkIsUtf8(data)\n",
    "    if not is_utf8:\n",
    "        print (\"Converting \" + infile + \" with \" + charenc + \" to UTF-8 and store in \" + outfile)\n",
    "        convertNWriteToUtf8(data, charenc, outfile)\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data conversion\n",
    "if CONVERT_DATA:\n",
    "    IS_DATASET1_CONVERTED = writeToUtf8(fin_set1, futf_set1)\n",
    "    IS_DATASET2_CONVERTED = writeToUtf8(fin_set2, futf_set2)\n",
    "    \n",
    "if IS_DATASET1_CONVERTED or READ_CONVERTED_DATASET1:\n",
    "    fset1 = futf_set1\n",
    "else: \n",
    "    fset1 = fin_set1\n",
    "    \n",
    "if IS_DATASET2_CONVERTED or READ_CONVERTED_DATASET2:\n",
    "    fset2 = futf_set2\n",
    "else: \n",
    "    fset2 = fin_set2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section provides an overview of the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acm = pd.read_csv(fset1, encoding='utf-8')\n",
    "df_acm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dblp2 = pd.read_csv(fset2, encoding='utf-8')\n",
    "df_dblp2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section cleans and prepares data to dedupe compliance format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(column):\n",
    "    \"\"\"\n",
    "    Do a little bit of data cleaning with the help of Unidecode and Regex.\n",
    "    Things like casing, extra spaces, quotes and new lines can be ignored.\n",
    "    \"\"\"\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = re.sub('-', '', column)\n",
    "    column = re.sub('/', ' ', column)\n",
    "    column = re.sub(\"'\", '', column)\n",
    "    column = re.sub(\",\", '', column)\n",
    "    column = re.sub(\":\", ' ', column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column :\n",
    "        column = None\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(filename):\n",
    "    \"\"\"\n",
    "    Read in our data from a CSV file and create a dictionary of records, \n",
    "    where the key is a unique record ID.\n",
    "    \"\"\"\n",
    "\n",
    "    data_d = {}\n",
    "    \n",
    "    with io.open(filename, 'rb') as f:\n",
    "        reader = csv.DictReader(f)             \n",
    "        for i, row in enumerate(reader):                 \n",
    "            #print row\n",
    "            clean_row = []\n",
    "            for k, v in row.items():                \n",
    "                if k == 'authors':\n",
    "                    proc_str = v.split(',')\n",
    "                    proc_str = tuple([preProcess(a) for a in proc_str])\n",
    "                    clean_row.append((k, proc_str))\n",
    "                else:\n",
    "                    clean_row.append((k, preProcess(v)))                    \n",
    "            #clean_row = dict([(k, preProcess(v)) for (k, v) in row.items()])            \n",
    "            data_d[filename + str(i)] = dict(clean_row)    \n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('importing data from ' + fset1 + ' ...')\n",
    "data_1 = readData(fset1)\n",
    "print('importing data from ' + fset2 + ' ...')\n",
    "data_2 = readData(fset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section performs record linkage. \n",
    "### Dedupe first trains a record linkage linker, then it performs blocking and matching and writes the result to a file.\n",
    "### To leverage previously trained data from active learning, write fout_training to the same file in every run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_fields():\n",
    "    # 'DBLP-ACM/ACM.csv585': {\n",
    "    #     'title': 'the tvtree an index structure for highdimensional data', \n",
    "    #     'authors': ('king ip lin', 'h. v. jagadish', 'christos faloutsos'), \n",
    "    #     'venue': 'the vldb journal &mdash; the international journal on very large data bases', \n",
    "    #     'id': '615210', \n",
    "    #     'year': '1994'}\n",
    "    # Define the fields the linker will pay attention to\n",
    "    #\n",
    "    # TODO:  use corpus for authors Set\n",
    "    fields = [\n",
    "            {'field' : 'title', 'type': 'String'},\n",
    "            {'field' : 'venue', 'type': 'String'},\n",
    "            {'field' : 'id', 'type': 'String'},\n",
    "            {'field' : 'year', 'type': 'ShortString'},\n",
    "            {'field' : 'authors', 'type': 'Set'}]\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fout_training, fout_settings, data_1, data_2):\n",
    "    if os.path.exists(fout_settings):\n",
    "        print('reading from', fout_settings)\n",
    "        with io.open(fout_settings, 'rb') as sf :\n",
    "            linker = dedupe.StaticRecordLink(sf)\n",
    "    else:\n",
    "        # Define fields format\n",
    "        fields = define_fields()\n",
    "\n",
    "        # Create a new linker object and pass our data model to it.\n",
    "        linker = dedupe.RecordLink(fields)\n",
    "\n",
    "        # To train the linker, we feed it a sample of records.\n",
    "        linker.sample(data_1, data_2, 15000)\n",
    "\n",
    "        # If we have training data saved from a previous run of linker,\n",
    "        # look for it an load it in.\n",
    "        # __Note:__ if you want to train from scratch, delete the training_file\n",
    "        if os.path.exists(fout_training):\n",
    "            print('reading labeled examples from ', fout_training)\n",
    "            with open(fout_training) as tf :\n",
    "                linker.readTraining(tf)\n",
    "\n",
    "        # ## Active learning\n",
    "        # Dedupe will find the next pair of records\n",
    "        # it is least certain about and ask you to label them as matches\n",
    "        # or not.\n",
    "        # use 'y', 'n' and 'u' keys to flag duplicates\n",
    "        # press 'f' when you are finished\n",
    "        print('starting active labeling...')\n",
    "\n",
    "        dedupe.consoleLabel(linker)\n",
    "\n",
    "        linker.train()\n",
    "\n",
    "        # When finished, save our training away to disk\n",
    "        with open(fout_training, 'w') as tf :\n",
    "            linker.writeTraining(tf)\n",
    "\n",
    "        # Save our weights and predicates to disk.  If the settings file\n",
    "        # exists, we will skip all the training and learning next time we run\n",
    "        # this file.\n",
    "        with open(fout_settings, 'wb') as sf :\n",
    "            linker.writeSettings(sf)\n",
    "            \n",
    "    return linker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blocking(linker, data1, data2):\n",
    "    # ## Blocking\n",
    "\n",
    "    # ## Clustering\n",
    "\n",
    "    # Find the threshold that will maximize a weighted average of our\n",
    "    # precision and recall.  When we set the recall weight to 2, we are\n",
    "    # saying we care twice as much about recall as we do precision.\n",
    "    #\n",
    "    # If we had more data, we would not pass in all the blocked data into\n",
    "    # this function but a representative sample.\n",
    "\n",
    "    print('clustering...')\n",
    "    linked_records = linker.match(data_1, data_2, 0)\n",
    "\n",
    "    print('# duplicate sets', len(linked_records))\n",
    "    return linked_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_result(fout_res, fset1, fset2, linked_records):\n",
    "    # ## Writing Results\n",
    "\n",
    "    # Write our original data back out to a CSV with a new column called \n",
    "    # 'Cluster ID' which indicates which records refer to each other.\n",
    "\n",
    "    cluster_membership = {}\n",
    "    cluster_id = None\n",
    "    for cluster_id, (cluster, score) in enumerate(linked_records):\n",
    "        for record_id in cluster:\n",
    "            cluster_membership[record_id] = (cluster_id, score)\n",
    "\n",
    "    if cluster_id :\n",
    "        unique_id = cluster_id + 1\n",
    "    else :\n",
    "        unique_id =0\n",
    "\n",
    "    with io.open(fout_res, 'wb') as f:\n",
    "        writer = csv.writer(f, encoding='utf-8')\n",
    "\n",
    "        header_unwritten = True\n",
    "\n",
    "        for fileno, filename in enumerate((fset1, fset2)) :\n",
    "            with io.open(filename, 'rb') as f_input :\n",
    "                reader = csv.reader(f_input, encoding='utf-8')\n",
    "\n",
    "                if header_unwritten :\n",
    "                    heading_row = next(reader)\n",
    "                    heading_row.insert(0, ('source file'))\n",
    "                    heading_row.insert(0, ('Link Score'))\n",
    "                    heading_row.insert(0, ('Cluster ID'))\n",
    "                    writer.writerow(heading_row)\n",
    "                    header_unwritten = False\n",
    "                else :\n",
    "                    next(reader)\n",
    "\n",
    "                for row_id, row in enumerate(reader):\n",
    "                    cluster_details = cluster_membership.get(filename + str(row_id))\n",
    "                    if cluster_details is None :\n",
    "                        cluster_id = unique_id\n",
    "                        unique_id += 1\n",
    "                        score = None\n",
    "                    else :\n",
    "                        cluster_id, score = cluster_details\n",
    "                    row.insert(0, fileno)\n",
    "                    row.insert(0, score)\n",
    "                    row.insert(0, cluster_id)\n",
    "                    writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_RUN):\n",
    "    [fout_res, fout_training, fout_settings] = getDedupeOutputFilename(UNIQUE_RUN_ID + i)\n",
    "    linker = train(fout_training, fout_settings, data_1, data_2)\n",
    "    linked_records = blocking(linker, data_1, data_2)\n",
    "    write_result(fout_res, fset1, fset2, linked_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluates record linkage results with ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ground_truth():\n",
    "    \"\"\"Populate ground truth\"\"\"\n",
    "    true_map = {}             # true_map        = { paper 1: paper 2   and paper 2: paper 1}\n",
    "    with io.open(fground, 'rb') as f:\n",
    "        reader = csv.DictReader(f)        \n",
    "        for i, row in enumerate(reader):                 \n",
    "            dblp = row.get('idDBLP')\n",
    "            acm  = row.get('idACM')\n",
    "            true_map[dblp] = acm\n",
    "            true_map[acm] = dblp\n",
    "    return true_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dedupe_result(fres):\n",
    "    res_dedupe = {}       # res_dedupe     = { pid : cluster_id }\n",
    "    cluster_dedupe = {}   # cluster_dedupe = { cluster_id: [paper 1, paper 2]}\n",
    "    res_map = {}          # res_map        = { paper 1: paper 2   and paper 2: paper 1}\n",
    "\n",
    "    with io.open(fres, 'rb') as csvfile:\n",
    "        datadict = csv.DictReader(csvfile)\n",
    "        for row in datadict:\n",
    "            pid = row.get(\"id\")\n",
    "            cluster_id = row.get(\"Cluster ID\")\n",
    "\n",
    "            # cluster ID for each paper\n",
    "            res_dedupe[pid] = cluster_id\n",
    "\n",
    "            # papers for each cluster\n",
    "            if not cluster_dedupe.get(cluster_id):\n",
    "                cluster_dedupe[cluster_id] = [pid]\n",
    "            else:\n",
    "                cluster_dedupe.get(cluster_id).append(pid)\n",
    "\n",
    "    # Get ACM mapped to DBLP and vice versa\n",
    "    for k, v in res_dedupe.items():\n",
    "        papers = cluster_dedupe.get(v)\n",
    "        if len(papers) == 1:\n",
    "            res_map[k] = ''\n",
    "        elif len(papers) == 2:\n",
    "            if papers.index(k) == 0:\n",
    "                res_map[k] = papers[1]            \n",
    "            else:\n",
    "                res_map[k] = papers[0]\n",
    "#         else: \n",
    "#             if k is not None and v is not None:\n",
    "#                 print (k, \" \", v)\n",
    "#                 print (\"More than two papers mapped in the same cluster\")\n",
    "    return res_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_true_false_positive_negative(true_map, res_map):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    for k, pred_v in res_map.items():    \n",
    "        if true_map.get(k) is not None:\n",
    "            true_v = true_map.get(k)        \n",
    "            if true_v == pred_v:\n",
    "                tp += 1            \n",
    "            else:\n",
    "                tn += 1        \n",
    "        else:\n",
    "            fp += 1\n",
    "            # TODO： what if results found sth which is wrong? fn?\n",
    "    return [tp, tn, fp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_precision_recall_score(true_map, res_map, mode):\n",
    "    true_arr = []\n",
    "    pred_arr = []\n",
    "    for k, pred_v in res_map.items():    \n",
    "        if true_map.get(k) is not None: \n",
    "            true_v = true_map.get(k)\n",
    "            true_arr.append(true_v)\n",
    "        else:\n",
    "            true_arr.append('')\n",
    "        pred_arr.append(pred_v)\n",
    "        \n",
    "    y_true = np.array(true_arr)\n",
    "    y_pred = np.array(pred_arr)\n",
    "\n",
    "    return precision_recall_fscore_support(y_true, y_pred, average=mode) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_arr = []\n",
    "true_map = read_ground_truth()\n",
    "for i in range(NUM_RUN):\n",
    "    fres_dedupe    = fout_dir + 'data_matching_output_' + str(UNIQUE_RUN_ID + i) + '.csv'\n",
    "    res_map        = read_dedupe_result(fres_dedupe)\n",
    "    [tp, tn, fp]   = analyse_true_false_positive_negative(true_map, res_map)\n",
    "    micro          = analyse_precision_recall_score(true_map, res_map, 'micro')\n",
    "    macro          = analyse_precision_recall_score(true_map, res_map, 'macro')\n",
    "    weighted       = analyse_precision_recall_score(true_map, res_map, 'weighted')\n",
    "    res = {}\n",
    "    res['set_id'] = UNIQUE_RUN_ID + i\n",
    "    res['true_positive'] = tp\n",
    "    res['true_negative'] = tn\n",
    "    res['false_positive'] = fp\n",
    "    res['micro_precision'] = micro[0]\n",
    "    res['micro_recall'] = micro[1]\n",
    "    res['micro_fscore'] = micro[2]\n",
    "    res['macro_precision'] = macro[0]\n",
    "    res['macro_recall'] = macro[1]\n",
    "    res['macro_fscore'] = macro[2]\n",
    "    res['weighted_precision'] = weighted[0]\n",
    "    res['weighted_recall'] = weighted[1]\n",
    "    res['weighted_fscore'] = weighted[2]\n",
    "    res_arr.append(res)\n",
    "    \n",
    "headernames = ['set_id', \n",
    "           'true_positive', 'true_negative', 'false_positive', \n",
    "           'micro_precision', 'micro_recall', 'micro_fscore',\n",
    "           'macro_precision', 'macro_recall', 'macro_fscore',\n",
    "           'weighted_precision', 'weighted_recall', 'weighted_fscore']\n",
    "    \n",
    "with io.open(fstats, 'wb') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=headernames)\n",
    "    writer.writeheader()\n",
    "    for r in res_arr:\n",
    "        writer.writerow(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section plots statistical results from the record linkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf = pd.read_csv(fstats,header=0,sep=',')\n",
    "# resdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_positive_negative_df = resdf[['true_positive', 'true_negative', 'false_positive']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_weighted_precision_recall_fscore_df = resdf[['weighted_precision', 'weighted_recall', 'weighted_fscore']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_precision_recall_fscore_df = resdf[['micro_precision', 'micro_recall', 'micro_fscore',\n",
    "                   'macro_precision', 'macro_recall', 'macro_fscore',\n",
    "                   'weighted_precision', 'weighted_recall', 'weighted_fscore']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_df = resdf[['micro_precision', 'macro_precision', 'weighted_precision']]\n",
    "new_names = {'micro_precision':'micro', 'macro_precision':'macro', 'weighted_precision':'weighted'}\n",
    "precision_df = precision_df.rename(index=str, columns=new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_df = resdf[['micro_recall', 'macro_recall', 'weighted_recall']]\n",
    "new_names = {'micro_recall':'micro', 'macro_recall':'macro', 'weighted_recall':'weighted'}\n",
    "recall_df = recall_df.rename(index=str, columns=new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fscore_df = resdf[['micro_fscore', 'macro_fscore', 'weighted_fscore']]\n",
    "new_names = {'micro_fscore':'micro', 'macro_fscore':'macro', 'weighted_fscore':'weighted'}\n",
    "fscore_df = fscore_df.rename(index=str, columns=new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_bar(indf, title, outfile, ylim=[-1,-1]):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    ax = sns.barplot(data=indf)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_xticks(ax.get_xticks()-0.5)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n",
    "    ax.set_ylabel('Average')\n",
    "    \n",
    "    if ylim[0] != -1 and ylim[1] != -1:\n",
    "        ax.set_ylim(ylim[0], ylim[1])\n",
    "\n",
    "    for p in ax.patches:\n",
    "         ax.annotate(\"%.2f\" % p.get_height(), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "             ha='center', va='center', fontsize=11, color='blue', rotation=90, xytext=(0, 20),\n",
    "             textcoords='offset points')\n",
    "\n",
    "    fig = ax.get_figure()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(fout_dir + outfile + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_bar(res_positive_negative_df, \"True Positve/Negative & False Positive\", \"mean_positive_negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_bar(res_weighted_precision_recall_fscore_df, \"Weighted Precision/Recall/F-score\", \"mean_weighted_precision_recall_fscore\", [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_bar(res_precision_recall_fscore_df, \"Precision/Recall/F-score\", \"mean_precision_recall_fscore\", [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = res_positive_negative_df\n",
    "\n",
    "def prep_df(df, name):\n",
    "    df = df.stack().reset_index()\n",
    "    df.columns = ['c1', 'c2', 'values']\n",
    "    df['DF'] = name\n",
    "    return df\n",
    "\n",
    "df1 = prep_df(df1, 'Count')\n",
    "\n",
    "df = pd.concat([df1])  \n",
    "# print (df)\n",
    "\n",
    "alt.enable_mime_rendering()\n",
    "\n",
    "alt.Chart(df).mark_bar().encode(\n",
    "\n",
    "    # tell Altair which field to group columns on\n",
    "    x=alt.X('c2:N',\n",
    "        axis=alt.Axis(\n",
    "            title='')),\n",
    "\n",
    "    # tell Altair which field to use as Y values and how to calculate\n",
    "    y=alt.Y('sum(values):Q',\n",
    "        axis=alt.Axis(\n",
    "            grid=False,\n",
    "            title='')),\n",
    "\n",
    "    # tell Altair which field to use to use as the set of columns to be represented in each group\n",
    "    column=alt.Column('c1:N',\n",
    "                 axis=alt.Axis(\n",
    "            title='')),\n",
    "\n",
    "    # tell Altair which field to use for color segmentation \n",
    "    color=alt.Color('DF:N',\n",
    "            scale=alt.Scale(\n",
    "                # make it look pretty with an enjoyable color pallet\n",
    "                range=['#96ceb4', '#ffcc5c','#ff6f69'],\n",
    "            ),\n",
    "        ))\\\n",
    "    .configure_facet_cell(\n",
    "    # remove grid lines around column clusters\n",
    "        strokeWidth=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = precision_df\n",
    "df2 = recall_df\n",
    "df3 = fscore_df\n",
    "\n",
    "def prep_df(df, name):\n",
    "    df = df.stack().reset_index()\n",
    "    df.columns = ['c1', 'c2', 'values']\n",
    "    df['DF'] = name\n",
    "    return df\n",
    "\n",
    "df1 = prep_df(df1, 'Precision')\n",
    "df2 = prep_df(df2, 'Recall')\n",
    "df3 = prep_df(df3, 'F-Score')\n",
    "\n",
    "df = pd.concat([df1, df2, df3])  \n",
    "# print (df)\n",
    "\n",
    "alt.enable_mime_rendering()\n",
    "\n",
    "alt.Chart(df).mark_bar().encode(\n",
    "\n",
    "    # tell Altair which field to group columns on\n",
    "    x=alt.X('c2:N',\n",
    "        axis=alt.Axis(\n",
    "            title='')),\n",
    "\n",
    "    # tell Altair which field to use as Y values and how to calculate\n",
    "    y=alt.Y('sum(values):Q',\n",
    "        axis=alt.Axis(\n",
    "            grid=False,\n",
    "            title='')),\n",
    "\n",
    "    # tell Altair which field to use to use as the set of columns to be represented in each group\n",
    "    column=alt.Column('c1:N',\n",
    "                 axis=alt.Axis(\n",
    "            title='')),\n",
    "\n",
    "    # tell Altair which field to use for color segmentation \n",
    "    color=alt.Color('DF:N',\n",
    "            scale=alt.Scale(\n",
    "                # make it look pretty with an enjoyable color pallet\n",
    "                range=['#96ceb4', '#ffcc5c','#ff6f69'],\n",
    "            ),\n",
    "        ))\\\n",
    "    .configure_facet_cell(\n",
    "    # remove grid lines around column clusters\n",
    "        strokeWidth=0.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section selects the best result and generates network graph\n",
    "\n",
    "#### The network graph consists of \n",
    "\n",
    "* Node Type: Author, Paper\n",
    "* Edge Type: writes, co-authored with, is same paper as, is same person as\n",
    "  - writes :  Author - writes - Paper\n",
    "  - co-authored with : Author - co-authored with - Author\n",
    "  - is same paper as : Paper - is same paper as - Paper\n",
    "  - is same person as : Author - is same person as - Author\n",
    "  \n",
    "* For 'is same person as', we first identify a pair of correctly matched (true positive) paper from both ACM and DBLP sets, then we map each author in the DBLP paper to at least 1 author in the ACM paper using LSH.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_run():\n",
    "    \"\"\"\n",
    "    get datasets with max true positive\n",
    "    \"\"\" \n",
    "    res = resdf[['set_id','true_positive']].sort_values('true_positive', ascending=False) \n",
    "    best_set_id = res.iloc[0]['set_id']\n",
    "    return fout_dir + 'data_matching_output_' + str(best_set_id) + '.csv'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkifstr(obj):\n",
    "    if isinstance(obj, float):\n",
    "        return False\n",
    "    else:\n",
    "        return bool(obj) and all(isinstance(elem, str) for elem in obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_token_seq(token_seq):\n",
    "    m_hash = MinHash(NUM_PERM)\n",
    "    for word in token_seq:\n",
    "        m_hash.update(word.encode('utf8'))\n",
    "    return m_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_index(lsh, mention_id, mention_tokens):\n",
    "    if not mention_id in lsh:\n",
    "        m_hash = has_token_seq(mention_tokens)\n",
    "        lsh.insert(mention_id, m_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_query(lsh, tokenized_query):\n",
    "    hash_query = has_token_seq(tokenized_query)\n",
    "    return lsh.query(hash_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hash a list of author\n",
    "def hash_data(data_ls, data_dict):\n",
    "    lsh = MinHashLSH(threshold=SIM_THRES, num_perm=NUM_PERM)\n",
    "    for a in data_ls:    \n",
    "        cleaned_token = a.split()  \n",
    "        mid = data_dict.get(a)\n",
    "        add_to_index(lsh, (mid,a), cleaned_token) \n",
    "    return lsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node_dict(node_id, node_typ, node_label, node_extref_id):\n",
    "    node_dict = {}\n",
    "    node_dict['Id'] = node_id\n",
    "    node_dict['Type'] = node_typ\n",
    "    node_dict['Label'] = node_label\n",
    "    node_dict['Extref_Id'] = node_extref_id\n",
    "    return node_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edge_dict(node1, rel, node2):\n",
    "    rel_dict = {}\n",
    "    rel_dict['Source'] = node1\n",
    "    rel_dict['Rel'] = rel\n",
    "    rel_dict['Target'] = node2\n",
    "    return rel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_similar_authors(dict_node_acm_author, acm_authors_proc, dict_node_dblp2_author, dblp2_authors_proc):\n",
    "    # create index with one author set, which is ACM author set for this one\n",
    "    hash_bucket = hash_data(acm_authors_proc, dict_node_acm_author)   \n",
    "    \n",
    "    # get a list of candidate for each author\n",
    "    authors_candidates = {}\n",
    "    for a in dblp2_authors_proc:\n",
    "        cleaned_token = a.split()\n",
    "        candidate_ls = send_query(hash_bucket, cleaned_token)\n",
    "        dblp2_id = dict_node_dblp2_author.get(a)\n",
    "        authors_candidates[(dblp2_id, a)] = candidate_ls\n",
    "    \n",
    "    # map all candidates to each author (instead of selecting one)\n",
    "    sim_authors = []\n",
    "    for k, v in authors_candidates.items():\n",
    "        node1 = k[0]\n",
    "        rel = 'is same person as'\n",
    "        for sv in v:\n",
    "            node2 = sv[0]\n",
    "            sim_authors.append(create_edge_dict(node1, rel, node2))\n",
    "    return sim_authors\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_similar_papers(dict_node_acm_title, acm_title_proc, dict_node_dblp2_title, dblp2_title_proc):\n",
    "    node1 = dict_node_acm_title.get(acm_title_proc)\n",
    "    rel = 'is same paper as'\n",
    "    node2 = dict_node_dblp2_title.get(dblp2_title_proc)\n",
    "    return ([create_edge_dict(node1, rel, node2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_coauthor_edge(authors_dict, authors_list):\n",
    "    co_authorship = []\n",
    "    for i in range(len(authors_list)):\n",
    "        for j in range(i+1, len(authors_list)):\n",
    "            node1 = authors_dict.get(authors_list[i])\n",
    "            node2 = authors_dict.get(authors_list[j])\n",
    "            rel = 'co-authored with'\n",
    "#             print (node1, \" co-authored with \", node2)\n",
    "            if node1 is not None and node2 is not None:\n",
    "                co_authorship.append(create_edge_dict(node1, rel, node2))\n",
    "            elif node1 is None:\n",
    "                print ('populate_coauthor_edge:', node1, ' not found')\n",
    "            elif node2 is None:\n",
    "                print ('populate_coauthor_edge:', node2, ' not found')\n",
    "    return co_authorship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_write_edge(authors_dict, authors_list, paper_dict, title_proc):\n",
    "    authoring = []\n",
    "    rel = 'writes'\n",
    "    node2 = paper_dict.get(title_proc)\n",
    "    for i in range(len(authors_list)):\n",
    "        node1 = authors_dict.get(authors_list[i])\n",
    "        if node1:\n",
    "            authoring.append(create_edge_dict(node1, rel, node2))\n",
    "        else:\n",
    "            print ('populate_write_edge:', authors_list[i], ' not found')\n",
    "    return authoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def populate_graph(res_map, true_map):\n",
    "    \n",
    "    #TODO: check EMPTY  res_map, true_map, df_acm, df_dblp2\n",
    "    \n",
    "    \n",
    "    # Get a list of author nodes (name, paper-id, node-typ)\n",
    "    df_acm_idx               = df_acm.set_index(['id'])\n",
    "    df_acm_idstr             = df_acm['id'].astype(str).tolist()\n",
    "    df_dblp2_idx             = df_dblp2.set_index(['id'])\n",
    "\n",
    "    global_id                = 0\n",
    "    NODE_TYP_AUTHOR_ACM      = 0\n",
    "    NODE_TYP_AUTHOR_DBLP2    = 1\n",
    "    NODE_TYP_PAPER_ACM       = 2\n",
    "    NODE_TYP_PAPER_DBLP2     = 3\n",
    "\n",
    "    dict_node                = {}     # map { id: original author or paper name}\n",
    "    dict_node_acm_author     = {}     # map { author_proc_name: id}\n",
    "    dict_node_dblp2_author   = {}\n",
    "    dict_node_acm_paper      = {}     # map { paper_proc_name: id}\n",
    "    dict_node_dblp2_paper    = {}     \n",
    "\n",
    "    node_arr                 = []     # to create nodes.csv (including author and paper)\n",
    "    author_paper_arr         = []     # to create 'author [writes] paper' rel. \n",
    "    coauthor_link_arr        = []     # to create 'author [co-authored with] author' rel.\n",
    "    is_same_author_link_arr  = []     # to create 'author [is same person as] author' rel. \n",
    "    is_same_paper_link_arr   = []     # to create 'paper [is same paper as] paper' rel. \n",
    "\n",
    "    for k, pred_v in res_map.items():\n",
    "        if true_map.get(k) is not None:\n",
    "            true_v = true_map.get(k)        \n",
    "            if true_v == pred_v:\n",
    "                if k in df_acm_idstr:   # Filter only one-way\n",
    "                    acm_authors   = df_acm_idx.loc[int(k)].authors\n",
    "                    dblp2_authors = df_dblp2_idx.loc[pred_v].authors\n",
    "                    acm_title     = df_acm_idx.loc[int(k)].title\n",
    "                    dblp2_title   = df_dblp2_idx.loc[pred_v].title\n",
    "\n",
    "                    # Create paper node\n",
    "                    proc_acm_title = preProcess(acm_title)\n",
    "                    dict_node_acm_paper[proc_acm_title] = global_id\n",
    "                    dict_node[global_id] = acm_title\n",
    "                    node_prop = create_node_dict(global_id, NODE_TYP_PAPER_ACM, acm_title, k)\n",
    "                    node_arr.append(node_prop)\n",
    "                    global_id += 1\n",
    "\n",
    "                    proc_dblp2_title = preProcess(dblp2_title)\n",
    "                    dict_node_dblp2_paper[proc_dblp2_title] = global_id\n",
    "                    dict_node[global_id] = dblp2_title\n",
    "                    node_prop = create_node_dict(global_id, NODE_TYP_PAPER_DBLP2, dblp2_title, k)\n",
    "                    node_arr.append(node_prop)\n",
    "                    global_id += 1\n",
    "\n",
    "                    # Populate co-author list\n",
    "                    acm_authors_proc = []\n",
    "                    dblp2_authors_proc = []\n",
    "\n",
    "                    # Create author node\n",
    "                    if checkifstr(acm_authors):\n",
    "                        for oa in acm_authors.split(','):\n",
    "                            a = preProcess(oa)\n",
    "                            acm_authors_proc.append(a)\n",
    "                            if a not in dict_node_acm_author:\n",
    "                                dict_node_acm_author[a] = global_id\n",
    "                                dict_node[global_id] = oa\n",
    "                                node_prop = create_node_dict(global_id, NODE_TYP_AUTHOR_ACM, oa, k)\n",
    "                                node_arr.append(node_prop)\n",
    "                                global_id += 1\n",
    "\n",
    "                    if checkifstr(dblp2_authors):\n",
    "                        for oa in dblp2_authors.split(','):\n",
    "                            a = preProcess(oa)\n",
    "                            dblp2_authors_proc.append(a)\n",
    "                            if a not in dict_node_dblp2_author:\n",
    "                                dict_node_dblp2_author[a] = global_id\n",
    "                                dict_node[global_id] = oa\n",
    "                                node_prop = create_node_dict(global_id, NODE_TYP_AUTHOR_DBLP2, oa, pred_v)\n",
    "                                node_arr.append(node_prop)\n",
    "                                global_id += 1\n",
    "\n",
    "                    # Populate co-author list\n",
    "                    edges_list = populate_coauthor_edge(dict_node_acm_author, acm_authors_proc)\n",
    "                    coauthor_link_arr.extend(edges_list)\n",
    "                    edges_list = populate_coauthor_edge(dict_node_dblp2_author, dblp2_authors_proc)\n",
    "                    coauthor_link_arr.extend(edges_list)\n",
    "\n",
    "                    # Link similar author in both lists\n",
    "                    edges_list = link_similar_authors(dict_node_acm_author, acm_authors_proc, dict_node_dblp2_author, dblp2_authors_proc)\n",
    "                    is_same_author_link_arr.extend(edges_list)\n",
    "\n",
    "                    # Link similar paper\n",
    "                    edge_list = link_similar_papers(dict_node_acm_paper, proc_acm_title, dict_node_dblp2_paper, proc_dblp2_title)\n",
    "                    is_same_paper_link_arr.extend(edge_list)\n",
    "\n",
    "                    # Populate authoring edge\n",
    "                    edges_list = populate_write_edge(dict_node_acm_author, acm_authors_proc, dict_node_acm_paper, proc_acm_title)\n",
    "                    author_paper_arr.extend(edges_list)\n",
    "                    edges_list = populate_write_edge(dict_node_dblp2_author, dblp2_authors_proc, dict_node_dblp2_paper, proc_dblp2_title)\n",
    "                    author_paper_arr.extend(edges_list)\n",
    "                    \n",
    "    return [node_arr, coauthor_link_arr, is_same_author_link_arr, is_same_paper_link_arr, author_paper_arr]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_graph(res_map, true_map):\n",
    "    [node_arr, coauthor_link_arr, is_same_author_link_arr, is_same_paper_link_arr, author_paper_arr] = populate_graph(res_map, true_map)\n",
    "    \n",
    "    snag = SNA_Graph()\n",
    "    nodels = []\n",
    "    edgels = []\n",
    "\n",
    "    for n in node_arr:\n",
    "        attr = {}\n",
    "        attr['Extref_Id'] = n.get('Extref_Id')\n",
    "        nodels.append(snag.create_node(str(n.get('Id')) + '_' + n.get('Label'), \n",
    "                                       n.get('Label'),\n",
    "                                       n.get('Type')))\n",
    "    for e in coauthor_link_arr:\n",
    "        src = e.get('Source')\n",
    "        dst = e.get('Target')\n",
    "        edgels.append(snag.create_edge(str(src) + '_' + dict_node.get(src), \n",
    "                                       str(dst) + '_' + dict_node.get(dst),\n",
    "                                       e.get('Rel')))\n",
    "    for e in is_same_author_link_arr:\n",
    "        src = e.get('Source')\n",
    "        dst = e.get('Target')\n",
    "        edgels.append(snag.create_edge(str(src) + '_' + dict_node.get(src), \n",
    "                                       str(dst) + '_' + dict_node.get(dst),\n",
    "                                       e.get('Rel')))\n",
    "\n",
    "    for e in is_same_paper_link_arr:\n",
    "        src = e.get('Source')\n",
    "        dst = e.get('Target')\n",
    "        edgels.append(snag.create_edge(str(src) + '_' + dict_node.get(src), \n",
    "                                       str(dst) + '_' + dict_node.get(dst),\n",
    "                                       e.get('Rel')))\n",
    "    for e in author_paper_arr:\n",
    "        src = e.get('Source')\n",
    "        dst = e.get('Target')\n",
    "        edgels.append(snag.create_edge(str(src) + '_' + dict_node.get(src), \n",
    "                                       str(dst) + '_' + dict_node.get(dst),\n",
    "                                       e.get('Rel')))\n",
    "\n",
    "    g_author = snag.create_graph('Author-Paper Network Graph', nodels, edgels)\n",
    "    nx.write_gml(g_author, author_paper_gml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GEN_GRAPH: \n",
    "    fres_dedupe = get_best_run()\n",
    "    true_map = read_ground_truth()\n",
    "    res_map  = read_dedupe_result(fres_dedupe)\n",
    "    gen_graph(res_map, true_map)\n",
    "else:\n",
    "    snag = nx.read_gml(author_paper_gml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section provides an overview of graph analytics.\n",
    "\n",
    "#### We first identify the number of connected component, we then dive into a selected connected component and analyse the network properties of this component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of connected component\n",
    "sna_cc = SNA_ConnectedComponent(snag)\n",
    "print (\"Number of connected component:\", sna_cc.get_number_of_connected_component())\n",
    "\n",
    "# Extract subgraph of connected component N\n",
    "connected_component_N = 3\n",
    "selected_cc = sna_cc.get_descending_sorted_connected_component_by_ranking(connected_component_N)\n",
    "print (\"Number of node in selected connected component:\", len(selected_cc))\n",
    "\n",
    "# Analyse subgraph (selected component) and write subgraph to GML\n",
    "sna_ana = SNA_Analytics(selected_cc, connected_component_gml)\n",
    "sna_ana.analyse_network()\n",
    "sna_ana.write_gml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check out http://holoviews.org/user_guide/Network_Graphs.html\n",
    "hv.extension('bokeh')\n",
    "%opts Graph [width=600 height=600]\n",
    "hv.Graph.from_networkx(selected_cc, nx.layout.spring_layout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
